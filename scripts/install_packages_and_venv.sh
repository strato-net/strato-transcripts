#!/bin/bash
# ==============================================================================
# Python Environment Setup Script for WhisperX Transcription
# ==============================================================================
#
# DESCRIPTION:
#   Automated setup script that installs and configures WhisperX for audio
#   transcription with speaker diarization. Supports multiple GPU vendors
#   and CPU-only mode for flexible hardware configurations including eGPU.
#
# OPERATING SYSTEM SUPPORT:
#   - macOS Sonoma (14.x)
#   - Ubuntu 24.04 LTS
#   Note: Script will fail immediately on any other OS or version
#
# HARDWARE SUPPORT:
#   - NVIDIA GPUs: RTX 5070 Blackwell, RTX 50/40/30/20 series, GTX, Tesla
#   - AMD GPUs: RX 6000/7000 series via ROCm
#   - Intel GPUs: Arc, Iris Xe, UHD Graphics via Intel Extension for PyTorch
#   - CPU-only: Fallback for any system
#
# WHAT IT DOES:
#   1. Detects OS and version
#   2. Creates isolated Python virtual environment for selected GPU vendor
#   3. Installs system dependencies (ffmpeg, build tools, Python dev)
#   4. Installs WhisperX and dependencies
#   5. Installs PyTorch for selected backend (CUDA/ROCm/XPU/CPU)
#   6. Verifies PyTorch installation
#   7. Applies compatibility patches to WhisperX
#   8. Installs pyannote.audio 4.0.1 (last release without torch==2.8.0 pin)
#   9. Applies compatibility patches to SpeechBrain
#  10. Verifies package installations
#  11. Sets up environment configuration file
#
# REQUIREMENTS:
#   - macOS Sonoma (14.x) OR Ubuntu 24.04 LTS
#   - Python 3.12
#   - macOS: Homebrew installed (script will check and guide installation)
#   - Ubuntu: sudo access for system package installation
#   - GPU drivers must be installed first:
#     - NVIDIA: run install_nvidia_drivers.sh
#     - AMD: run install_amd_drivers.sh
#     - Intel: run install_intel_drivers.sh
#
# USAGE:
#   ./install_packages_and_venv.sh --nvidia   # NVIDIA GPU (creates venv-nvidia)
#   ./install_packages_and_venv.sh --amd      # AMD GPU via ROCm (creates venv-amd)
#   ./install_packages_and_venv.sh --intel    # Intel GPU via XPU (creates venv-intel)
#   ./install_packages_and_venv.sh --cpu      # CPU-only (creates venv-cpu)
#   ./install_packages_and_venv.sh --all      # Create all four venvs
#
# eGPU WORKFLOW:
#   Pre-create all venvs, then activate the right one based on connected GPU:
#     source venv-nvidia/bin/activate   # NVIDIA eGPU connected
#     source venv-amd/bin/activate      # AMD eGPU connected
#     source venv-intel/bin/activate    # Intel iGPU fallback
#     source venv-cpu/bin/activate      # Pure CPU fallback
#
# POST-INSTALLATION:
#   1. Get HuggingFace token: https://huggingface.co/settings/tokens
#   2. Edit setup_env.sh and add your token
#   3. Accept model agreements:
#      - https://huggingface.co/pyannote/speaker-diarization-3.1
#      - https://huggingface.co/pyannote/segmentation-3.0
#
# TROUBLESHOOTING:
#   - NVIDIA: If nvidia-smi fails, reboot after driver installation
#   - AMD: Set HSA_OVERRIDE_GFX_VERSION=10.3.0 for unofficial GPUs (RX 6600/6700/6750)
#   - Intel: Ensure libze-intel-gpu1 and intel-opencl-icd are installed
#   - If imports fail: Ensure virtual environment is activated
#
# ==============================================================================

set -e  # Exit immediately if any command fails

# Terminal color codes for formatted output
RED='\033[0;31m'       # Error messages
GREEN='\033[0;32m'     # Success messages
YELLOW='\033[1;33m'    # Warning/info messages
BLUE='\033[0;34m'      # Section headers
NC='\033[0m'           # No Color (reset)

# Project directories - resolved to absolute paths
# Script is in ./scripts/, so go up one level to project root
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(cd "$SCRIPT_DIR/.." && pwd)"  # Project root (parent of scripts/)

# Parse command-line arguments
GPU_MODE=""
INSTALL_ALL=false

show_usage() {
    echo "Usage: $0 [--nvidia|--amd|--intel|--cpu|--all]"
    echo ""
    echo "Options:"
    echo "  --nvidia    Create venv-nvidia with CUDA PyTorch"
    echo "  --amd       Create venv-amd with ROCm PyTorch"
    echo "  --intel     Create venv-intel with Intel XPU PyTorch"
    echo "  --cpu       Create venv-cpu with CPU-only PyTorch"
    echo "  --all       Create all four venvs"
    echo ""
    echo "At least one option is required."
}

for arg in "$@"; do
    case $arg in
        --nvidia)
            GPU_MODE="nvidia"
            shift
            ;;
        --amd)
            GPU_MODE="amd"
            shift
            ;;
        --intel)
            GPU_MODE="intel"
            shift
            ;;
        --cpu|--force-cpu)
            GPU_MODE="cpu"
            shift
            ;;
        --all)
            INSTALL_ALL=true
            shift
            ;;
        *)
            echo -e "${RED}Error: Unknown option: $arg${NC}"
            show_usage
            exit 1
            ;;
    esac
done

# Validate arguments
if [ "$INSTALL_ALL" = false ] && [ -z "$GPU_MODE" ]; then
    echo -e "${RED}Error: No GPU mode specified${NC}"
    show_usage
    exit 1
fi

# Function to set venv directory based on mode
set_venv_dir() {
    local mode=$1
    case $mode in
        nvidia) VENV_DIR="$PROJECT_DIR/venv-nvidia" ;;
        amd)    VENV_DIR="$PROJECT_DIR/venv-amd" ;;
        intel)  VENV_DIR="$PROJECT_DIR/venv-intel" ;;
        cpu)    VENV_DIR="$PROJECT_DIR/venv-cpu" ;;
    esac
}

echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}Python Environment Setup for WhisperX${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""

# ==============================================================================
# Step 0: OS Detection and Validation
# ==============================================================================
# Detect the operating system and version.
# Only macOS Sonoma (14.x) and Ubuntu 24.04 LTS are supported.
# Script will exit immediately if run on any other OS or version.
# ==============================================================================
echo -e "${YELLOW}[0/15] Detecting operating system...${NC}"

OS_TYPE=""
OS_VERSION=""

if [[ "$OSTYPE" == "darwin"* ]]; then
    OS_TYPE="macos"
    # Get macOS version
    OS_VERSION=$(sw_vers -productVersion)
    OS_MAJOR=$(echo "$OS_VERSION" | cut -d. -f1)
    
    echo "Detected macOS version: $OS_VERSION"
    
    if [ "$OS_MAJOR" -ne 14 ]; then
        echo -e "${RED}ERROR: Unsupported macOS version${NC}"
        echo "This script requires macOS Sonoma (14.x)"
        echo "Your version: $OS_VERSION"
        exit 1
    fi
    
    echo -e "${GREEN}✓ macOS Sonoma detected${NC}"
    
    # Check for Homebrew
    if ! command -v brew &> /dev/null; then
        echo -e "${RED}ERROR: Homebrew not installed${NC}"
        echo "Homebrew is required for macOS installations."
        echo "Install it from: https://brew.sh"
        echo "Then run this script again."
        exit 1
    fi
    echo -e "${GREEN}✓ Homebrew is installed${NC}"
    
elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
    OS_TYPE="ubuntu"
    
    # Check if it's Ubuntu
    if [ ! -f /etc/os-release ]; then
        echo -e "${RED}ERROR: Cannot detect Linux distribution${NC}"
        exit 1
    fi
    
    source /etc/os-release
    
    if [ "$ID" != "ubuntu" ]; then
        echo -e "${RED}ERROR: Unsupported Linux distribution${NC}"
        echo "This script requires Ubuntu 24.04 LTS"
        echo "Your distribution: $ID $VERSION_ID"
        exit 1
    fi
    
    echo "Detected Ubuntu version: $VERSION_ID"
    
    if [ "$VERSION_ID" != "24.04" ]; then
        echo -e "${RED}ERROR: Unsupported Ubuntu version${NC}"
        echo "This script requires Ubuntu 24.04 LTS"
        echo "Your version: $VERSION_ID"
        exit 1
    fi
    
    echo -e "${GREEN}✓ Ubuntu 24.04 LTS detected${NC}"
    
else
    echo -e "${RED}ERROR: Unsupported operating system${NC}"
    echo "This script requires either:"
    echo "  - macOS Sonoma (14.x)"
    echo "  - Ubuntu 24.04 LTS"
    exit 1
fi

echo ""

# ==============================================================================
# Step 1: GPU Mode Selection
# ==============================================================================
# Shows selected GPU mode and verifies driver availability.
# GPU mode is specified via command-line flags (--nvidia, --amd, --intel, --cpu).
# ==============================================================================
echo -e "${YELLOW}[1/15] GPU mode selection...${NC}"

show_mode_info() {
    local mode=$1
    case $mode in
        nvidia)
            echo -e "${BLUE}Mode: NVIDIA (CUDA)${NC}"
            echo "Will create: venv-nvidia"
            echo "PyTorch: 2.9.1+cu130"
            if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
                GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null || echo "NVIDIA GPU")
                echo -e "${GREEN}✓ Detected: $GPU_NAME${NC}"
            else
                echo -e "${YELLOW}⚠ No NVIDIA GPU currently detected (eGPU may be disconnected)${NC}"
            fi
            ;;
        amd)
            echo -e "${BLUE}Mode: AMD (ROCm)${NC}"
            echo "Will create: venv-amd"
            echo "PyTorch: 2.6.0+rocm6.2"
            if command -v rocminfo &> /dev/null; then
                echo -e "${GREEN}✓ ROCm tools available${NC}"
            else
                echo -e "${YELLOW}⚠ ROCm not detected (run install_amd_drivers.sh first)${NC}"
            fi
            ;;
        intel)
            echo -e "${BLUE}Mode: Intel (XPU)${NC}"
            echo "Will create: venv-intel"
            echo "PyTorch: 2.5.1 + IPEX 2.5.10+xpu"
            if [ -f /usr/lib/x86_64-linux-gnu/libze_loader.so.1 ]; then
                echo -e "${GREEN}✓ Level Zero available${NC}"
            else
                echo -e "${YELLOW}⚠ Intel XPU runtime not detected (run install_intel_drivers.sh first)${NC}"
            fi
            ;;
        cpu)
            echo -e "${BLUE}Mode: CPU-only${NC}"
            echo "Will create: venv-cpu"
            echo "PyTorch: 2.9.1 (CPU)"
            echo "No GPU acceleration - fallback mode"
            ;;
    esac
}

if [ "$OS_TYPE" = "macos" ]; then
    # macOS only supports CPU/MPS mode
    if [ "$GPU_MODE" != "cpu" ] && [ "$INSTALL_ALL" = false ]; then
        echo -e "${YELLOW}⚠ macOS only supports CPU mode (with MPS acceleration)${NC}"
        GPU_MODE="cpu"
    fi
fi

if [ "$INSTALL_ALL" = true ]; then
    echo "Will create all venvs: venv-nvidia, venv-amd, venv-intel, venv-cpu"
else
    show_mode_info "$GPU_MODE"
fi
echo ""

# ==============================================================================
# Step 2: System Dependencies and AI Tools Installation
# ==============================================================================
# Install required system packages using appropriate package manager.
# macOS: Uses Homebrew (brew)
# Ubuntu: Uses apt package manager
# These are low-level dependencies needed to build Python packages and process audio.
# ==============================================================================
echo -e "${YELLOW}[2/15] Installing system dependencies...${NC}"

if [ "$OS_TYPE" = "macos" ]; then
    echo "Installing required packages via Homebrew:"
    echo "  - ffmpeg: Audio/video processing for WhisperX"
    echo "  - python@3.12: Python 3.12 interpreter"
    echo "  - git: Version control for installing packages from GitHub"
    
    # Install packages if not already present
    brew list ffmpeg &>/dev/null || brew install ffmpeg
    brew list python@3.12 &>/dev/null || brew install python@3.12
    brew list git &>/dev/null || brew install git
    
    # Set Python 3.12 from Homebrew as the python3 command
    echo "Setting up Python 3.12 from Homebrew..."
    export PATH="/opt/homebrew/opt/python@3.12/libexec/bin:$PATH"
    
    # Verify we're using the correct Python version
    DETECTED_PY_VERSION=$(python3 --version)
    echo "Using: $DETECTED_PY_VERSION"
    
    echo -e "${GREEN}✓ System dependencies installed${NC}"
    
elif [ "$OS_TYPE" = "ubuntu" ]; then
    echo "Installing required system packages:"
    echo "  - build-essential: C/C++ compilers for building Python packages"
    echo "  - ca-certificates: SSL/TLS certificates for secure connections"
    echo "  - curl: HTTP client for API requests"
    echo "  - ffmpeg: Audio/video processing for WhisperX"
    echo "  - git: Version control for installing packages from GitHub"
    echo "  - libcurl4-openssl-dev: cURL development libraries for Python packages"
    echo "  - libssl-dev: SSL development libraries"
    echo "  - python3-dev: Python headers for compiling extensions"
    echo "  - python3-pip: Python package installer"
    echo "  - python3-venv: Python virtual environment support"
    
    sudo apt update
    sudo apt install -y \
      build-essential \
      ca-certificates \
      curl \
      ffmpeg \
      git \
      libcurl4-openssl-dev \
      libssl-dev \
      python3-dev \
      python3-pip \
      python3-venv
    
    echo -e "${GREEN}✓ System dependencies installed${NC}"
fi
echo ""

# ==============================================================================
# Determine which modes to install
# ==============================================================================
if [ "$INSTALL_ALL" = true ]; then
    MODES_TO_INSTALL="nvidia amd intel cpu"
    echo -e "${BLUE}Installing all GPU backends...${NC}"
else
    MODES_TO_INSTALL="$GPU_MODE"
fi

# ==============================================================================
# Main installation loop - install for each selected mode
# ==============================================================================
for CURRENT_MODE in $MODES_TO_INSTALL; do

echo ""
echo -e "${BLUE}======================================================${NC}"
echo -e "${BLUE}Installing for: $CURRENT_MODE${NC}"
echo -e "${BLUE}======================================================${NC}"
echo ""

# Set venv directory for current mode
set_venv_dir "$CURRENT_MODE"

# ==============================================================================
# Step 4: Python Virtual Environment Creation
# ==============================================================================
# Create an isolated Python environment to avoid conflicts with system packages.
# If venv already exists, it's removed and recreated to ensure clean state.
# All subsequent Python packages will be installed into this venv.
# ==============================================================================
echo -e "${YELLOW}[4/15] Creating Python virtual environment...${NC}"
echo "Creating isolated Python environment to avoid conflicts with system packages"
echo "Location: $VENV_DIR"
if [ -d "$VENV_DIR" ]; then
    echo -e "${YELLOW}Warning: venv directory already exists. Removing...${NC}"
    # Try normal removal first, fall back to sudo only if permission denied
    if ! rm -rf "$VENV_DIR" 2>/dev/null; then
        echo -e "${YELLOW}Permission denied - trying with sudo...${NC}"
        echo "Note: This shouldn't be necessary. The venv may have been created with sudo previously."
        sudo rm -rf "$VENV_DIR"
    fi
fi
python3 -m venv "$VENV_DIR"
echo -e "${GREEN}✓ Virtual environment created${NC}"
echo ""

# Activate the virtual environment for all subsequent pip installations
source "$VENV_DIR/bin/activate"

# ==============================================================================
# Step 5: Install Base Packages
# ==============================================================================
# Installs WhisperX, AI provider SDKs, and dependencies from requirements.txt.
# WhisperX will pull PyTorch 2.8.0, which we'll upgrade in the next step.
# Includes transcription service (AssemblyAI)
# and post-processing services (Anthropic, Google Gemini).
# ==============================================================================
echo -e "${YELLOW}[5/15] Installing base packages...${NC}"
echo "Installing WhisperX, AI provider SDKs, and dependencies from requirements.txt"
echo "Note: WhisperX will pull PyTorch 2.8.0 (we'll upgrade to 2.9.1 next)"
echo "This may take 5-10 minutes..."
pip install -r "$PROJECT_DIR/requirements.txt"
echo -e "${GREEN}✓ Base packages installed${NC}"
echo ""

# ==============================================================================
# Step 6: PyTorch Installation for Selected Backend
# ==============================================================================
# Installs PyTorch for the selected GPU backend:
#   - NVIDIA: PyTorch 2.9.1+cu130 (CUDA 13.0, Blackwell support)
#   - AMD: PyTorch 2.6.0+rocm6.2 (ROCm)
#   - Intel: PyTorch 2.5.1 + IPEX 2.5.10+xpu (Intel Extension for PyTorch)
#   - CPU: PyTorch 2.9.1 (CPU-only)
# Uses --force-reinstall to ensure correct variant is installed.
# ==============================================================================
echo -e "${YELLOW}[6/15] Installing PyTorch for $CURRENT_MODE backend...${NC}"
echo "This may take 2-5 minutes depending on internet speed..."

case $CURRENT_MODE in
    nvidia)
        echo "Installing PyTorch 2.9.1 with CUDA 13.0 support"
        echo "Provides full Blackwell (sm_120) support for RTX 50-series GPUs"
        pip install --force-reinstall --index-url https://download.pytorch.org/whl/cu130 \
            torch==2.9.1 \
            torchvision==0.24.1 \
            torchaudio==2.9.1
        echo -e "${GREEN}✓ PyTorch 2.9.1+cu130 installed${NC}"
        ;;
    amd)
        echo "Installing PyTorch 2.6.0 with ROCm 6.2 support"
        pip install --force-reinstall --index-url https://download.pytorch.org/whl/rocm6.2 \
            torch==2.6.0 \
            torchvision==0.21.0 \
            torchaudio==2.6.0
        echo -e "${GREEN}✓ PyTorch 2.6.0+rocm6.2 installed${NC}"
        ;;
    intel)
        echo "Installing PyTorch 2.5.1 with Intel XPU support (IPEX)"
        pip install --force-reinstall \
            torch==2.5.1 \
            torchvision \
            torchaudio \
            intel-extension-for-pytorch==2.5.10+xpu \
            --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
        echo -e "${GREEN}✓ PyTorch 2.5.1 + IPEX 2.5.10+xpu installed${NC}"
        ;;
    cpu)
        if [ "$OS_TYPE" = "macos" ]; then
            echo "Installing PyTorch 2.9.1 with MPS (Metal) support"
            pip install --force-reinstall \
                torch==2.9.1 \
                torchaudio==2.9.1
            echo -e "${GREEN}✓ PyTorch 2.9.1 (MPS) installed${NC}"
        else
            echo "Installing PyTorch 2.9.1 CPU-only"
            pip install --force-reinstall --index-url https://download.pytorch.org/whl/cpu \
                torch==2.9.1 \
                torchvision==0.24.1 \
                torchaudio==2.9.1
            echo -e "${GREEN}✓ PyTorch 2.9.1+cpu installed${NC}"
        fi
        ;;
esac
echo ""

# ==============================================================================
# Step 7: PyTorch Verification
# ==============================================================================
# Verifies PyTorch installation and hardware accessibility for selected backend.
# ==============================================================================
echo -e "${YELLOW}[7/15] Verifying PyTorch installation...${NC}"
echo "Verifying PyTorch installation for $CURRENT_MODE backend..."

case $CURRENT_MODE in
    nvidia)
        python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\"}')"

        # Only test GPU if one is currently connected (eGPU may be disconnected)
        if python3 -c "import torch; exit(0 if torch.cuda.is_available() else 1)" 2>/dev/null; then
            echo "Testing GPU operations..."
            python3 -c "import torch; x = torch.randn(100,100, device='cuda'); print('✓ GPU test passed:', x.matmul(x).sum().item())"

            echo "Testing cuDNN..."
            python3 -c "import torch.backends.cudnn as cudnn; print('✓ cuDNN version:', cudnn.version()); print('✓ cuDNN enabled:', cudnn.is_available())"

            echo -e "${GREEN}✓ PyTorch verified - NVIDIA GPU ready${NC}"
        else
            echo -e "${YELLOW}⚠ No NVIDIA GPU currently detected (eGPU may be disconnected)${NC}"
            echo "PyTorch CUDA support installed - will work when GPU is connected"
        fi
        ;;
    amd)
        python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'ROCm available: {torch.cuda.is_available()}'); print(f'HIP version: {torch.version.hip if hasattr(torch.version, \"hip\") else \"N/A\"}')"

        if python3 -c "import torch; exit(0 if torch.cuda.is_available() else 1)" 2>/dev/null; then
            echo "Testing GPU operations..."
            python3 -c "import torch; x = torch.randn(100,100, device='cuda'); print('✓ GPU test passed:', x.matmul(x).sum().item())"
            echo -e "${GREEN}✓ PyTorch verified - AMD GPU ready${NC}"
        else
            echo -e "${YELLOW}⚠ No AMD GPU currently detected (eGPU may be disconnected)${NC}"
            echo "PyTorch ROCm support installed - will work when GPU is connected"
            echo "Note: For unofficial GPUs (RX 6600/6700/6750), set HSA_OVERRIDE_GFX_VERSION=10.3.0"
        fi
        ;;
    intel)
        python3 -c "import torch; import intel_extension_for_pytorch as ipex; print(f'PyTorch: {torch.__version__}'); print(f'IPEX: {ipex.__version__}'); print(f'XPU available: {torch.xpu.is_available()}')" 2>/dev/null || \
        python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print('IPEX import failed')"

        if python3 -c "import torch; import intel_extension_for_pytorch; exit(0 if torch.xpu.is_available() else 1)" 2>/dev/null; then
            echo "Testing XPU operations..."
            python3 -c "import torch; import intel_extension_for_pytorch; x = torch.randn(100,100, device='xpu'); print('✓ XPU test passed:', x.sum().item())"
            echo -e "${GREEN}✓ PyTorch verified - Intel XPU ready${NC}"
        else
            echo -e "${YELLOW}⚠ Intel XPU not available${NC}"
            echo "Ensure libze-intel-gpu1 and intel-opencl-icd are installed"
        fi
        ;;
    cpu)
        python3 -c "import torch; print(f'PyTorch: {torch.__version__}')"
        echo "Testing CPU operations..."
        python3 -c "import torch; x = torch.randn(100,100); print('✓ CPU test passed:', x.matmul(x).sum().item())"

        if [ "$OS_TYPE" = "macos" ]; then
            echo "Testing MPS (Metal) availability..."
            python3 -c "import torch; print('✓ MPS available:', torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False)"
        fi

        echo -e "${GREEN}✓ PyTorch verified - CPU ready${NC}"
        ;;
esac
echo ""

# ==============================================================================
# Step 8: WhisperX Compatibility Patches
# ==============================================================================
# Updates WhisperX source code to use 'token' parameter for HuggingFace authentication.
# This enables compatibility with pyannote.audio 4.x API.
# Patches are applied with sed and verified.
# Files modified: vads/pyannote.py (global replace) and asr.py (line 412).
# Note: sed syntax differs between macOS and Linux
# ==============================================================================
echo -e "${YELLOW}[8/15] Applying WhisperX patches...${NC}"
echo "Updating WhisperX to use 'token' parameter for HuggingFace authentication"
echo "Enables compatibility with pyannote.audio 4.x"

# Find the actual site-packages directory (handles any Python version)
SITE_PACKAGES=$(python3 -c "import site; print(site.getsitepackages()[0])")
WHISPERX_VADS="$SITE_PACKAGES/whisperx/vads/pyannote.py"
WHISPERX_ASR="$SITE_PACKAGES/whisperx/asr.py"

if [ ! -f "$WHISPERX_VADS" ]; then
    echo -e "${RED}ERROR: WhisperX vads/pyannote.py not found at $WHISPERX_VADS${NC}"
    exit 1
fi

if [ ! -f "$WHISPERX_ASR" ]; then
    echo -e "${RED}ERROR: WhisperX asr.py not found at $WHISPERX_ASR${NC}"
    exit 1
fi

# Apply patches (handling sed differences between macOS and Linux)
if [ "$OS_TYPE" = "macos" ]; then
    sed -i '' 's/use_auth_token/token/g' "$WHISPERX_VADS"
    sed -i '' '412s/use_auth_token=None/token=None/' "$WHISPERX_ASR"
else
    sed -i 's/use_auth_token/token/g' "$WHISPERX_VADS"
    sed -i '412s/use_auth_token=None/token=None/' "$WHISPERX_ASR"
fi

# Verify patches
VADS_COUNT=$(grep -c "use_auth_token" "$WHISPERX_VADS" || true)
if [ "$VADS_COUNT" -ne 0 ]; then
    echo -e "${RED}ERROR: Patch verification failed for vads/pyannote.py${NC}"
    exit 1
fi

echo -e "${GREEN}✓ WhisperX patches applied successfully${NC}"
echo ""

# ==============================================================================
# Step 9: pyannote.audio Installation
# ==============================================================================
# Installs pyannote.audio 4.0.1, the last release that doesn't hard-pin torch==2.8.0.
# Versions 4.0.2+ force torch 2.8.0, which conflicts with our upgrade to torch 2.9.1.
# ==============================================================================
echo -e "${YELLOW}[9/15] Installing pyannote.audio 4.0.1...${NC}"
echo "Installing pyannote.audio==4.0.1 (4.0.2+ pin torch==2.8.0 and conflict with Blackwell torch 2.9.1)..."
pip install --upgrade "pyannote.audio==4.0.1"
echo -e "${GREEN}✓ pyannote.audio 4.0.1 installed${NC}"
echo ""

# ==============================================================================
# Step 10: SpeechBrain Compatibility Patches
# ==============================================================================
# Updates SpeechBrain to work with torchaudio 2.9.1's API.
# Adds hasattr() check to gracefully handle different torchaudio versions.
# This ensures SpeechBrain can detect available audio backends across versions.
# ==============================================================================
echo -e "${YELLOW}[10/15] Applying SpeechBrain compatibility patches...${NC}"
echo "Updating SpeechBrain for torchaudio 2.9.1 compatibility"
echo "Adding version-agnostic audio backend detection"

SPEECHBRAIN_BACKEND="$SITE_PACKAGES/speechbrain/utils/torch_audio_backend.py"

if [ ! -f "$SPEECHBRAIN_BACKEND" ]; then
    echo -e "${RED}ERROR: SpeechBrain torch_audio_backend.py not found at $SPEECHBRAIN_BACKEND${NC}"
    exit 1
fi

# Create the patch - adds hasattr() check for list_audio_backends()
cat > /tmp/speechbrain_patch.py << 'PATCH_EOF'
import sys

# Read the file
with open(sys.argv[1], 'r') as f:
    content = f.read()

# Check if already patched
if 'hasattr(torchaudio, \'list_audio_backends\')' in content:
    # Verify the structure is correct
    if 'if hasattr(torchaudio, \'list_audio_backends\'):\n        available_backends = torchaudio.list_audio_backends()\n        if len(available_backends)' in content.replace('    ', ' '*4):
        print("Already patched with correct structure")
        sys.exit(0)
    else:
        print("Patch exists but structure incorrect - re-patching")

# Apply the patch - find and replace the problematic section
original = """    elif torchaudio_major >= 2 and torchaudio_minor >= 1:
        available_backends = torchaudio.list_audio_backends()

        if len(available_backends) == 0:
            logger.warning(
                "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html"
            )"""

replacement = """    elif torchaudio_major >= 2 and torchaudio_minor >= 1:
        # list_audio_backends() is not available in torchaudio 2.9.1
        if hasattr(torchaudio, 'list_audio_backends'):
            available_backends = torchaudio.list_audio_backends()
            if len(available_backends) == 0:
                logger.warning(
                    "SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html"
                )
        else:
            # Newer torchaudio versions don't have list_audio_backends()
            logger.info("Using torchaudio with default audio backend")"""

if original in content:
    content = content.replace(original, replacement)
    with open(sys.argv[1], 'w') as f:
        f.write(content)
    print("Patch applied successfully")
else:
    # Try alternative matching for hasattr case
    if 'hasattr(torchaudio, \'list_audio_backends\')' in content:
        print("Already patched")
    else:
        print("ERROR: Could not find pattern to patch")
        sys.exit(1)
PATCH_EOF

# Apply the patch
python3 /tmp/speechbrain_patch.py "$SPEECHBRAIN_BACKEND"

# Verify the patch
if grep -q "hasattr(torchaudio, 'list_audio_backends')" "$SPEECHBRAIN_BACKEND"; then
    echo -e "${GREEN}✓ SpeechBrain compatibility patch applied successfully${NC}"
else
    echo -e "${RED}ERROR: SpeechBrain patch verification failed${NC}"
    exit 1
fi

# Cleanup
rm -f /tmp/speechbrain_patch.py
echo ""

# ==============================================================================
# Step 10b: SpeechBrain dataio.py Patch for torchaudio.info removal
# ==============================================================================
# torchaudio 2.9.x removed torchaudio.info() and torchaudio.backend.common.AudioMetaData.
# This patch replaces read_audio_info() with a compatible implementation that uses
# torchaudio.load() to get audio metadata instead.
# ==============================================================================
echo -e "${YELLOW}[10b/15] Applying SpeechBrain dataio.py patch for torchaudio 2.9.x...${NC}"
echo "Patching read_audio_info() to work without torchaudio.info()"

SPEECHBRAIN_DATAIO="$SITE_PACKAGES/speechbrain/dataio/dataio.py"

if [ ! -f "$SPEECHBRAIN_DATAIO" ]; then
    echo -e "${RED}ERROR: SpeechBrain dataio.py not found at $SPEECHBRAIN_DATAIO${NC}"
    exit 1
fi

# Create the patch
cat > /tmp/speechbrain_dataio_patch.py << 'PATCH_EOF'
import sys

# Read the file
with open(sys.argv[1], 'r') as f:
    content = f.read()

# Check if already patched
if 'AudioMetaDataCompat' in content:
    print("Already patched")
    sys.exit(0)

# Define the compatibility class and replacement function
compat_class = '''
# Compatibility shim for torchaudio 2.9.x which removed AudioMetaData
class AudioMetaDataCompat:
    """Compatibility class replacing torchaudio.backend.common.AudioMetaData."""
    def __init__(self, sample_rate, num_frames, num_channels, bits_per_sample=16, encoding="PCM_S"):
        self.sample_rate = sample_rate
        self.num_frames = num_frames
        self.num_channels = num_channels
        self.bits_per_sample = bits_per_sample
        self.encoding = encoding

'''

# Find where to insert the class (after the imports, before read_audio_info)
# Look for the line before read_audio_info function
marker = "\ndef read_audio_info("
if marker not in content:
    print("ERROR: Could not find read_audio_info function")
    sys.exit(1)

# Insert the compat class before read_audio_info
content = content.replace(marker, compat_class + marker)

# Now replace the read_audio_info function
old_func_start = "def read_audio_info(\n    path, backend=None\n) -> \"torchaudio.backend.common.AudioMetaData\":"
new_func_start = "def read_audio_info(\n    path, backend=None\n) -> AudioMetaDataCompat:"

if old_func_start in content:
    content = content.replace(old_func_start, new_func_start)
else:
    print("WARNING: Could not update return type annotation")

# Replace the function body - find from docstring to return
# We need to replace the torchaudio.info calls with torchaudio.load
old_body = '''    validate_backend(backend)

    _path_no_ext, path_ext = os.path.splitext(path)

    if path_ext == ".mp3":
        # Additionally, certain affected versions of torchaudio fail to
        # autodetect mp3.
        # HACK: here, we check for the file extension to force mp3 detection,
        # which prevents an error from occurring in torchaudio.
        info = torchaudio.info(path, format="mp3", backend=backend)
    else:
        info = torchaudio.info(path, backend=backend)

    # Certain file formats, such as MP3, do not provide a reliable way to
    # query file duration from metadata (when there is any).
    # For MP3, certain versions of torchaudio began returning num_frames == 0.
    #
    # https://github.com/speechbrain/speechbrain/issues/1925
    # https://github.com/pytorch/audio/issues/2524
    #
    # Accommodate for these cases here: if `num_frames == 0` then maybe something
    # has gone wrong.
    # If some file really had `num_frames == 0` then we are not doing harm
    # double-checking anyway. If I am wrong and you are reading this comment
    # because of it: sorry
    if info.num_frames == 0:
        channels_data, sample_rate = torchaudio.load(
            path, normalize=False, backend=backend
        )

        info.num_frames = channels_data.size(1)
        info.sample_rate = sample_rate  # because we might as well

    return info'''

new_body = '''    # torchaudio 2.9.x compatibility: use torchaudio.load() instead of removed torchaudio.info()
    if hasattr(torchaudio, 'info'):
        # Old torchaudio version - use original approach
        validate_backend(backend)
        _path_no_ext, path_ext = os.path.splitext(path)
        if path_ext == ".mp3":
            info = torchaudio.info(path, format="mp3", backend=backend)
        else:
            info = torchaudio.info(path, backend=backend)
        if info.num_frames == 0:
            channels_data, sample_rate = torchaudio.load(path, normalize=False, backend=backend)
            info.num_frames = channels_data.size(1)
            info.sample_rate = sample_rate
        return info
    else:
        # torchaudio 2.9.x: info() removed, use load() to get metadata
        # Note: backend parameter is ignored in torchaudio 2.9.x
        channels_data, sample_rate = torchaudio.load(path, normalize=False)
        return AudioMetaDataCompat(
            sample_rate=sample_rate,
            num_frames=channels_data.size(1),
            num_channels=channels_data.size(0),
        )'''

if old_body in content:
    content = content.replace(old_body, new_body)
    with open(sys.argv[1], 'w') as f:
        f.write(content)
    print("Patch applied successfully")
else:
    print("WARNING: Could not find exact function body to patch")
    print("Attempting alternative patch...")
    # If exact match fails, at least add the compat class was added
    with open(sys.argv[1], 'w') as f:
        f.write(content)
    print("Partial patch applied (compat class added)")
PATCH_EOF

# Apply the patch
python3 /tmp/speechbrain_dataio_patch.py "$SPEECHBRAIN_DATAIO"

# Verify the patch
if grep -q "AudioMetaDataCompat" "$SPEECHBRAIN_DATAIO"; then
    echo -e "${GREEN}✓ SpeechBrain dataio.py patch applied successfully${NC}"
else
    echo -e "${RED}ERROR: SpeechBrain dataio.py patch verification failed${NC}"
    exit 1
fi

# Cleanup
rm -f /tmp/speechbrain_dataio_patch.py
echo ""

# ==============================================================================
# Step 10c: Lightning cloud_io.py Patch for PyTorch 2.6+ weights_only default
# ==============================================================================
# PyTorch 2.6+ changed torch.load default to weights_only=True for security.
# Pyannote/lightning models use pickle which requires weights_only=False.
# This patch sets weights_only=False for local file loading in lightning.
# ==============================================================================
echo -e "${YELLOW}[10c/15] Applying Lightning patch for PyTorch 2.6+ weights_only...${NC}"
echo "Patching lightning cloud_io.py to default weights_only=False for local files"

LIGHTNING_CLOUD_IO="$SITE_PACKAGES/lightning/fabric/utilities/cloud_io.py"

if [ ! -f "$LIGHTNING_CLOUD_IO" ]; then
    echo -e "${RED}ERROR: Lightning cloud_io.py not found at $LIGHTNING_CLOUD_IO${NC}"
    exit 1
fi

# Check if already patched
if grep -q "PyTorch 2.6+ compatibility patch" "$LIGHTNING_CLOUD_IO"; then
    echo "Already patched"
else
    # Apply patch - set default weights_only=False for local files
    sed -i 's/fs.open(path_or_url, "rb") as f:/fs.open(path_or_url, "rb") as f:\n        if weights_only is None:\n            weights_only = False  # PyTorch 2.6+ compatibility patch/' "$LIGHTNING_CLOUD_IO"

    # Verify the patch
    if grep -q "PyTorch 2.6+ compatibility patch" "$LIGHTNING_CLOUD_IO"; then
        echo -e "${GREEN}✓ Lightning cloud_io.py patch applied successfully${NC}"
    else
        echo -e "${RED}ERROR: Lightning cloud_io.py patch verification failed${NC}"
        exit 1
    fi
fi
echo ""

# ==============================================================================
# Step 11: LD_LIBRARY_PATH Configuration (project-specific via setup_env.sh)
# ==============================================================================
# Ensures setup_env.sh includes LD_LIBRARY_PATH for CUDA libraries
# Project-specific (only active when setup_env.sh is sourced)
# NOT added to ~/.bashrc to avoid global conflicts with other tools
# ==============================================================================
echo -e "${YELLOW}[11/15] Configuring LD_LIBRARY_PATH in setup_env.sh${NC}"
echo "Adding CUDA library paths to setup_env.sh (project-specific, not global)"

if [ -f "$PROJECT_DIR/setup_env.sh" ]; then
    # Check if LD_LIBRARY_PATH already configured
    if grep -q "LD_LIBRARY_PATH.*nvidia/cudnn" "$PROJECT_DIR/setup_env.sh"; then
        echo "✓ LD_LIBRARY_PATH already configured in setup_env.sh"
    else
        echo "Updating setup_env.sh to include LD_LIBRARY_PATH..."
        # Note: This should already be in the template, but this is a fallback
        echo "⚠ LD_LIBRARY_PATH not found - it should be in setup_env.sh.example template"
    fi
else
    echo "⚠ setup_env.sh not found - will be created in Step 14"
fi
echo ""

# ==============================================================================
# Step 12: Application Package Verification
# ==============================================================================
# Verifies WhisperX and pyannote.audio can be imported successfully.
# Import tests confirm all dependencies are properly installed and accessible.
# This validation ensures the environment is ready for transcription tasks.
# ==============================================================================
echo -e "${YELLOW}[12/15] Verifying package installations...${NC}"
echo "Testing imports to ensure all packages are properly installed and accessible"

echo "Testing WhisperX import..."
python3 -c "import whisperx; print('✓ WhisperX imported successfully')"

echo "Testing pyannote.audio import..."
python3 -c "from pyannote.audio import Pipeline; print('✓ pyannote.audio imported successfully')"

echo -e "${GREEN}✓ All packages verified and ready to use${NC}"
echo ""

# ==============================================================================
# Step 13: Verify AI Provider SDKs
# ==============================================================================
# Verifies that AI provider SDKs were installed from requirements.txt.
# These packages were already installed in Step 5 along with WhisperX.
# ==============================================================================
echo -e "${YELLOW}[13/15] Verifying AI provider SDKs...${NC}"
echo "Verifying packages installed from requirements.txt:"
echo "  Cloud transcription: assemblyai"
echo "  AI post-processing: anthropic, google-generativeai"
echo "  Utilities: requests"

# Test key imports
python3 -c "import assemblyai; print('✓ assemblyai')"
python3 -c "import anthropic; print('✓ anthropic')"
python3 -c "import google.generativeai; print('✓ google-generativeai')"
python3 -c "import requests; print('✓ requests')"

echo -e "${GREEN}✓ All AI provider SDKs verified${NC}"
echo ""

# ==============================================================================
# Step 14: Create Project Directories
# ==============================================================================
# Create intermediates and outputs directories for transcript processing.
# Ethereum glossaries (people/terms) are now generated on-demand during
# post-processing rather than at install time, as they're only needed then.
# ==============================================================================
echo -e "${YELLOW}[14/15] Creating project directories...${NC}"
echo "Creating project directory structure..."
mkdir -p "$PROJECT_DIR/intermediates"
mkdir -p "$PROJECT_DIR/outputs"
echo "✓ Created intermediates/ and outputs/ directories"
echo ""
echo "Note: Ethereum glossaries will be generated when needed during post-processing"
echo "      Run scripts/extract_people.py or extract_terms.py manually if desired"
echo ""

# ==============================================================================
# Step 15: Environment File Setup
# ==============================================================================
# Creates setup_env.sh from template if needed.
# This file stores the HuggingFace token for downloading pyannote models.
# User provides their token manually (see post-installation instructions).
# ==============================================================================
echo -e "${YELLOW}[15/15] Setting up environment configuration...${NC}"
echo "Checking for setup_env.sh (required for HuggingFace authentication)"
if [ ! -f "$PROJECT_DIR/setup_env.sh" ]; then
    if [ -f "$PROJECT_DIR/setup_env.sh.example" ]; then
        cp "$PROJECT_DIR/setup_env.sh.example" "$PROJECT_DIR/setup_env.sh"
        echo -e "${GREEN}✓ Created setup_env.sh from example template${NC}"
        echo "You'll need to edit this file to add your HuggingFace token"
    else
        echo -e "${YELLOW}Warning: setup_env.sh.example not found${NC}"
        echo "You'll need to create setup_env.sh manually"
    fi
else
    echo "setup_env.sh already exists - skipping creation"
fi
echo ""

echo -e "${GREEN}✓ Completed installation for: $CURRENT_MODE${NC}"
echo ""

done  # End of main installation loop

# Final success message
echo -e "${BLUE}========================================${NC}"
echo -e "${GREEN}✓ All Installations Complete!${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""

# Show what was installed
echo -e "${BLUE}Installed venvs:${NC}"
for mode in $MODES_TO_INSTALL; do
    case $mode in
        nvidia) echo "  venv-nvidia/  - NVIDIA GPU (CUDA)" ;;
        amd)    echo "  venv-amd/     - AMD GPU (ROCm)" ;;
        intel)  echo "  venv-intel/   - Intel GPU (XPU)" ;;
        cpu)    echo "  venv-cpu/     - CPU-only fallback" ;;
    esac
done
echo ""

echo -e "${YELLOW}MANUAL CONFIGURATION REQUIRED:${NC}"
echo ""
echo "1. Get a HuggingFace token:"
echo "   https://huggingface.co/settings/tokens"
echo ""
echo "2. Edit setup_env.sh and add your token:"
if [ "$OS_TYPE" = "macos" ]; then
    echo "   nano setup_env.sh  (or use your preferred editor)"
else
    echo "   nano setup_env.sh"
fi
echo ""
echo "3. Accept model agreements:"
echo "   - https://huggingface.co/pyannote/speaker-diarization-3.1"
echo "   - https://huggingface.co/pyannote/segmentation-3.0"
echo ""
echo -e "${YELLOW}OPTIONAL: Get API keys for cloud AI providers${NC}"
echo ""
echo "4. For cloud-based AI post-processing:"
echo "   - Anthropic (Claude Opus): https://console.anthropic.com/"
echo "   - Google (Gemini): https://makersuite.google.com/app/apikey"
echo ""
echo -e "${GREEN}Ready to use!${NC}"
echo ""
echo "Usage (activate the venv matching your current GPU):"
echo "  source setup_env.sh"
echo "  source venv-nvidia/bin/activate   # NVIDIA eGPU"
echo "  source venv-amd/bin/activate      # AMD eGPU"
echo "  source venv-intel/bin/activate    # Intel iGPU"
echo "  source venv-cpu/bin/activate      # CPU fallback"
echo ""
echo "Then run:"
echo "  ./scripts/process_single.sh audio.mp3 --transcribers whisperx --processors opus"
echo ""
echo "Available transcribers: whisperx, whisperx-cloud, assemblyai"
echo "Available processors: opus, gemini, deepseek, chatgpt"
echo ""
echo "Batch Processing:"
echo "  ./scripts/process_all.sh --transcribers assemblyai --processors opus"
echo ""
echo "See README.md and AI_PROVIDERS_GUIDE.md for complete documentation."
echo ""
